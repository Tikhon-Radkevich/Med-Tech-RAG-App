{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf8c1794504d38c",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:29.034588Z",
     "start_time": "2025-04-01T08:16:27.968994Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command, Any\n",
    "\n",
    "from constants import (\n",
    "    DATA_DIR,\n",
    "    EMBEDDINGS_MODEL_NAME,\n",
    "    INDEX_CACHE_DIR,\n",
    "    TOGETHER_META_LLAMA_70B_FREE,\n",
    "    TOGETHER_DEEPSEEK_DISTILL_LLAMA_70B_FREE,\n",
    "    TOGETHER_META_LLAMA_VISION_FREE,\n",
    "    GROUND_TRUTH_ACTUAL_JSON,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bcf5efd874d96b02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:29.055566Z",
     "start_time": "2025-04-01T08:16:29.047088Z"
    }
   },
   "source": [
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ad93a3b786ca8201",
   "metadata": {},
   "source": "## Load Docs"
  },
  {
   "cell_type": "code",
   "id": "b99bf1d14b0f2b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:45.502890Z",
     "start_time": "2025-04-01T08:16:45.499208Z"
    }
   },
   "source": [
    "dirs_to_explore = [\"LIFEPAK 15\", \"LIFEPAK 20\"]\n",
    "\n",
    "device_with_pdf_file_name = []\n",
    "pdf_files_to_explore = []\n",
    "for pdf_data_dir in dirs_to_explore:\n",
    "    pdf_files_path = os.path.join(DATA_DIR, pdf_data_dir)\n",
    "    pdf_files_to_explore += [\n",
    "        os.path.join(pdf_files_path, pdf_file_name)\n",
    "        for pdf_file_name in os.listdir(pdf_files_path)\n",
    "    ]\n",
    "    device_with_pdf_file_name += [\n",
    "        (pdf_data_dir.lower(), pdf_file_name)\n",
    "        for pdf_file_name in os.listdir(pdf_files_path)\n",
    "    ]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9446ff3c5ee7ca2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:47.296577Z",
     "start_time": "2025-04-01T08:16:47.291324Z"
    }
   },
   "source": [
    "def load_docs(pdf_files, device_with_pdf_title_metadata):\n",
    "    docs = []\n",
    "\n",
    "    for file_path, (device, pdf_title) in tqdm(\n",
    "        zip(pdf_files, device_with_pdf_title_metadata), total=len(pdf_files)\n",
    "    ):\n",
    "        loader = PDFPlumberLoader(file_path)\n",
    "        metadata = {\"device\": device, \"pdf_title\": pdf_title}\n",
    "        for doc in loader.load():\n",
    "            doc.metadata.update(metadata)\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(\"total docs:\", len(docs))\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "8a924627a7c287e6",
   "metadata": {},
   "source": "## Build Index"
  },
  {
   "cell_type": "code",
   "id": "3372e8ba91e1d5b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:48.154675Z",
     "start_time": "2025-04-01T08:16:48.151555Z"
    }
   },
   "source": [
    "index_name = \"LIFEPAK_index\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "52931a6dd43c86e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:58.166765Z",
     "start_time": "2025-04-01T08:16:48.693773Z"
    }
   },
   "source": [
    "def get_vectorstore(embeddings_model_name, collection_name, persist_directory):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    if os.path.exists(persist_directory):\n",
    "        vstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        print(\"Successfully loaded vectorstore!\")\n",
    "    else:\n",
    "        print(\"Failed to load vectorstore. Creating new one...\")\n",
    "        documents = load_docs(pdf_files_to_explore, device_with_pdf_file_name)\n",
    "        embeddings.show_progress = True\n",
    "        vstore = Chroma.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        embeddings.show_progress = False\n",
    "\n",
    "    return vstore\n",
    "\n",
    "\n",
    "vectorstore = get_vectorstore(EMBEDDINGS_MODEL_NAME, index_name, INDEX_CACHE_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded vectorstore!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "40e68e4ff900ed61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:16:58.696011Z",
     "start_time": "2025-04-01T08:16:58.257718Z"
    }
   },
   "source": [
    "res = vectorstore.similarity_search(\n",
    "    query=\"How do I troubleshoot Dampened Waveform on Lifepak 15?\", k=1\n",
    ")[0].page_content\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIFEPAKÂ®20e Defibrillator/Monitor\n",
      "Performance Inspection Procedure (PIP)\n",
      "PIP - 5-Lead ECG Tests\n",
      "Note: Perform this test if 5-lead ECG cable is used. Otherwise, skip to the next test.\n",
      "The 5-Lead ECG tests consist of:\n",
      "PIP - 5-Lead ECG Leads Off Detection Test\n",
      "PIP - 5-Lead ECG Gain Test\n",
      "PIP - 5-Lead ECG Leads Off Detection Test\n",
      "1. Connect the 5-wire ECG cable between the device and Impulse 7000DP as shown in Figure 1.12.\n",
      "2. Set the Impulse 7000DP output to a 1-mv, 10-Hz sine wave.\n",
      "3. Set the device Lead selection to LEAD II.\n",
      "4. Remove the RL lead from the Impulse 7000DP, and verify the device displays an ECG LEADS\n",
      "OFF message and a repeating priority 3 tones shall sound when the Lead is removed. Reconnect\n",
      "the RL lead.\n",
      "5. Remove the RA lead from the Impulse 7000DP, and verify the device displays an RA LEADS\n",
      "OFF message and a repeating priority 3 tones shall sound when the Lead is removed. Reconnect\n",
      "the RA lead.\n",
      "6. Remove the LL lead from the Impulse 7000DP, and verify the device displays an LL LEADS\n",
      "OFF message and a repeating priority 3 tones shall sound when the Lead is removed. Reconnect\n",
      "the LL lead.\n",
      "7. Set the device lead selection to LEAD I.\n",
      "8. Remove the LA lead from the Impulse 7000DP.\n",
      "9. Verify the device displays an LA LEADS OFF message and a repeating priority 3 tones shall sound\n",
      "when the Lead is removed. Reconnect the LA lead.\n",
      "10. Set the device LEAD selection to C LEAD.\n",
      "11. Remove the C1/V1 Lead from the Impulse 7000DP.\n",
      "Figure 1.12: 5 Lead ECG Leads\n",
      "12. Verify the device displays a C LEADS OFF message and a repeating priority 3 tones shall sound\n",
      "Test Setup\n",
      "when the Lead is removed. Reconnect the C1/V1 lead.\n",
      "13. Continue to the next test with this setup in place.\n",
      "3201896-000_C Â©2020 Stryker --- CONTENTS --- Page 22 of 61\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "7e6bb5eb74d80854",
   "metadata": {},
   "source": "## Build RAG"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dce46b2ccc7fdd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T17:18:51.367569Z",
     "start_time": "2025-03-30T17:18:51.363942Z"
    }
   },
   "outputs": [],
   "source": [
    "# def format_docs(docs):\n",
    "#     # return {\"context\": \"\\n\\n\".join(doc.page_content for doc in docs)}\n",
    "#     return {\"context\": \"\\n\\n\".join(dummy_doc for dummy_doc in [\"doc1\", \"doc2\", \"doc3\"])}\n",
    "#\n",
    "#\n",
    "# with open(GROUND_TRUTH_ACTUAL_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "#     ground_truth_actual_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b92d83e028cd27da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T17:18:51.430616Z",
     "start_time": "2025-03-30T17:18:51.407103Z"
    }
   },
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever()\n",
    "#\n",
    "# configurable_retriever = retriever.configurable_fields(\n",
    "#     search_kwargs=ConfigurableField(\n",
    "#         id=\"search_kwargs\",\n",
    "#         name=\"Search Kwargs\",\n",
    "#         description=\"The search kwargs to use\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "id": "13f10b3bfc300357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:12.643394Z",
     "start_time": "2025-04-01T08:17:12.640533Z"
    }
   },
   "source": [
    "# dummy_config = {\n",
    "#     \"configurable\": {\"search_kwargs\": {\"k\": 10, \"filter\": {\"device\": \"lifepak 20\"}}}\n",
    "# }\n",
    "#\n",
    "# dummy_res = configurable_retriever.invoke(\n",
    "#     \"How do I troubleshoot Dampened Waveform\", config=dummy_config\n",
    "# )\n",
    "# dummy_res[0].metadata"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "137534f6bd39a2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:15.970996Z",
     "start_time": "2025-04-01T08:17:15.967566Z"
    }
   },
   "source": [
    "# rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Also return response with provided structure, including used context and answer.\n",
    "# \"\"\"\n",
    "# rag_user_prompt = \"\"\"Question: {question}\n",
    "# Context: {context}\n",
    "# \"\"\"\n",
    "#\n",
    "#\n",
    "# class ContextUnit(BaseModel):\n",
    "#     title: str = Field(description=\"The title of pdf document.\")\n",
    "#     pages: list[int] = Field(description=\"The pages in pdf document.\")\n",
    "#\n",
    "#\n",
    "# class RetrievalOutput(BaseModel):\n",
    "#     context: list[ContextUnit] = Field(description=\"Used context documents and pages.\")\n",
    "#     answer: str = Field(\n",
    "#         description=\"The answer to the question.\",\n",
    "#     )\n",
    "#\n",
    "#\n",
    "# class VectorStoreInputKwargs(BaseModel):\n",
    "#     k: int = 5\n",
    "#     filter: dict[str, str] = None\n",
    "#\n",
    "#\n",
    "# class State(BaseModel):\n",
    "#     question: str = Field(description=\"The question text.\")\n",
    "#     context: list[Document] = Field(default=None, description=\"The context Documents.\")\n",
    "#     retriever_out: RetrievalOutput = Field(\n",
    "#         default=None, description=\"The retriever formatted output.\"\n",
    "#     )\n",
    "#     vectorstore_input_kwargs: VectorStoreInputKwargs = Field(\n",
    "#         default=None, description=\"The vectorstore input kwargs.\"\n",
    "#     )\n",
    "#\n",
    "#\n",
    "# llm = init_chat_model(model=TOGETHER_META_LLAMA_70B_FREE, temperature=0)\n",
    "# rag_structured = llm.with_structured_output(RetrievalOutput)\n",
    "#\n",
    "#\n",
    "# # Define application steps\n",
    "# def retrieve(state: State):\n",
    "#     retrieved_docs = vectorstore.similarity_search(\n",
    "#         state.question, **state.vectorstore_input_kwargs.model_dump()\n",
    "#     )\n",
    "#     return {\"context\": retrieved_docs}\n",
    "#\n",
    "#\n",
    "# def generate(state: State):\n",
    "#     docs_content = \"\\n\\n\".join(\n",
    "#         f\"Source Doc: {doc.metadata['pdf_title']}\\n\"\n",
    "#         # f\"Page: {doc.metadata['page']}\\n\"\n",
    "#         f\"{doc.page_content}\"\n",
    "#         for doc in state.context\n",
    "#     )\n",
    "#     response = rag_structured.invoke(\n",
    "#         [\n",
    "#             {\"role\": \"system\", \"content\": rag_system_prompt},\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": rag_user_prompt.format(\n",
    "#                     question=state.question, context=docs_content\n",
    "#                 ),\n",
    "#             },\n",
    "#         ]\n",
    "#     )\n",
    "#     return {\"retriever_out\": response}\n",
    "#\n",
    "#\n",
    "# # input_variables = {\"context\": configurable_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "# # prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# # llm = init_chat_model(model=TOGETHER_META_LLAMA_90B_FREE, temperature=0)\n",
    "# #\n",
    "# # rag_chain = input_variables | prompt | llm"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "664bd16e41f7c7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T17:39:08.534218Z",
     "start_time": "2025-03-30T17:39:08.529094Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "# graph_builder.add_edge(START, \"retrieve\")\n",
    "# graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e644d055db7b4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T17:39:13.012477Z",
     "start_time": "2025-03-30T17:39:08.978756Z"
    }
   },
   "outputs": [],
   "source": [
    "# initial_state = State(\n",
    "#     question=\"How do I troubleshoot low volume on Lifepak 20?\",\n",
    "#     vectorstore_input_kwargs=VectorStoreInputKwargs(\n",
    "#         filter={\"device\": \"lifepak 20\"}, k=5\n",
    "#     ),\n",
    "# )\n",
    "#\n",
    "#\n",
    "# result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "id": "383772cbc9ca0a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:18.217910Z",
     "start_time": "2025-04-01T08:17:18.214941Z"
    }
   },
   "source": "# generate.__name__",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "681264d0f42ff701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:19.319032Z",
     "start_time": "2025-04-01T08:17:19.316247Z"
    }
   },
   "source": [
    "# rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "# \"\"\"\n",
    "#\n",
    "# ChatPromptTemplate(\n",
    "#     [\n",
    "#         (\n",
    "#             \"user\",\n",
    "#             \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "#         ),\n",
    "#     ]\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "99ac832d443c031a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:20.315550Z",
     "start_time": "2025-04-01T08:17:20.312128Z"
    }
   },
   "source": [
    "# a = (\n",
    "#     configurable_retriever\n",
    "#     | format_docs\n",
    "#     | RunnablePassthrough().assign(copy_context=lambda x: x[\"context\"])\n",
    "#     | RunnablePassthrough()\n",
    "# )\n",
    "# a.invoke(\"hi\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "18406a69dbdd18ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:20.860348Z",
     "start_time": "2025-04-01T08:17:20.857407Z"
    }
   },
   "source": [
    "# example_idx = 1\n",
    "#\n",
    "# print(\"Q:\", ground_truth_actual_data[example_idx][\"question\"])\n",
    "# print(\"Expected:\", ground_truth_actual_data[example_idx][\"ground_truth\"])\n",
    "# print(\n",
    "#     \"\\nRAG Response:\",\n",
    "#     rag_chain.invoke(ground_truth_actual_data[example_idx][\"question\"]).content,\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32e29fc33229834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T12:44:08.304220Z",
     "start_time": "2025-03-30T12:44:08.297521Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt_args = {\n",
    "#     \"instructions\": {\"devices\": \",\".join(dummy_full_list_of_devices_lower)},\n",
    "#     \"rules\": {\n",
    "#         \"continue_rule\": \"1) device name is clear; \\n2) it is popular abbreviation, like LP - LIFEPAK.\",\n",
    "#         \"specify_rule\": \"1) there is no series number specification.\",\n",
    "#     },\n",
    "# }\n",
    "#\n",
    "# triage_system_prompt = \"\"\"\n",
    "# < Role >\n",
    "# You are technical devices medical assistant.\n",
    "# </ Role >\n",
    "#\n",
    "# < Instructions >\n",
    "#\n",
    "# You have access to list of devices:\n",
    "# {devices}\n",
    "#\n",
    "# Clients ask you a question about instructions about devices. Your first goal is specify exact title of device, to make filtering search in the future.\n",
    "#\n",
    "# Here you have two options:\n",
    "# 1. CONTINUE - if question contains clear title of device, including series number, and this device is form your accessible list.\n",
    "# 2. SPECIFY - if device name is unclear.\n",
    "#\n",
    "# Classify the below question into one of these categories. And specify exact device name from list.\n",
    "#\n",
    "# </ Instructions >\n",
    "#\n",
    "# < Rules >\n",
    "# You can continue with device name if:\n",
    "# {continue_rule}\n",
    "#\n",
    "# Ask user to specify device name if:\n",
    "# {specify_rule}\n",
    "# </ Rules >\n",
    "#\n",
    "# < Few shot examples >\n",
    "# {examples}\n",
    "# </ Few shot examples >\n",
    "# \"\"\"\n",
    "# var: str = \"hey\"\n",
    "# dev = \"lifepak 15\"\n",
    "#\n",
    "#\n",
    "# class DeviceClassificationRouter(BaseModel):\n",
    "#     \"\"\"Analyze the question and extract device name.\"\"\"\n",
    "#\n",
    "#     reasoning: str = Field(\n",
    "#         description=\"Step-by-step reasoning behind the classification.\"\n",
    "#     )\n",
    "#     device: Literal[*dummy_full_list_of_devices_lower, \"none\"] = Field(\n",
    "#         description=\"Exact available device names. Use 'none' if device name is unclear.\"\n",
    "#     )\n",
    "#     classification: Literal[\"continue\", \"specify\"] = Field(\n",
    "#         description=\"The classification of device: 'continue' weather device is specified, 'specify' if device unclear or no device in question.\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93342d5ee4ff9596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T12:44:08.926959Z",
     "start_time": "2025-03-30T12:44:08.798480Z"
    }
   },
   "outputs": [],
   "source": [
    "# # TOGETHER_META_LLAMA_VISION_FREE\n",
    "# # TOGETHER_META_LLAMA_90B_FREE\n",
    "# device_extractor_llm = init_chat_model(\n",
    "#     model=TOGETHER_META_LLAMA_70B_FREE, temperature=0\n",
    "# )\n",
    "# device_extractor_llm_router = device_extractor_llm.with_structured_output(\n",
    "#     DeviceClassificationRouter\n",
    "# )\n",
    "#\n",
    "# system_prompt = triage_system_prompt.format(\n",
    "#     devices=prompt_args[\"instructions\"][\"devices\"],\n",
    "#     continue_rule=prompt_args[\"rules\"][\"continue_rule\"],\n",
    "#     specify_rule=prompt_args[\"rules\"][\"specify_rule\"],\n",
    "#     examples=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b4b141ad2d9864f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T12:44:09.151755Z",
     "start_time": "2025-03-30T12:44:09.149088Z"
    }
   },
   "outputs": [],
   "source": [
    "# result = device_extractor_llm_router.invoke(\n",
    "#     [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": \"lifepak: how do I troubleshoot Dampened Waveform?\"},\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b34ee028d0e97c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T12:44:09.578552Z",
     "start_time": "2025-03-30T12:44:09.575345Z"
    }
   },
   "outputs": [],
   "source": [
    "# class State(TypedDict):\n",
    "#     k: int\n",
    "#     question: str\n",
    "#     device: str\n",
    "#     config: dict\n",
    "#     response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68ffb6c6b1d94571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T12:44:10.202887Z",
     "start_time": "2025-03-30T12:44:10.196081Z"
    }
   },
   "outputs": [],
   "source": [
    "# def medical_device_retriever_router(state: State) -> Command[Literal[\"__end__\"]]:\n",
    "#     question = state[\"question\"]\n",
    "#\n",
    "#     res = device_extractor_llm_router.invoke(\n",
    "#         [\n",
    "#             {\"role\": \"system\", \"content\": system_prompt},\n",
    "#             {\"role\": \"user\", \"content\": question},\n",
    "#         ]\n",
    "#     )\n",
    "#\n",
    "#     if (res.classification == \"continue\") and (\n",
    "#         res.device in prompt_args[\"instructions\"][\"devices\"]\n",
    "#     ):\n",
    "#         print(\"Continue with retrieval agent.\")\n",
    "#         config = {\n",
    "#             \"configurable\": {\n",
    "#                 \"search_kwargs\": {\"k\": state[\"k\"], \"filter\": {\"device\": res.device}}\n",
    "#             }\n",
    "#         }\n",
    "#         response = rag_chain.invoke(input=question, config=config)\n",
    "#         update = {\"device\": res.device, \"config\": config, \"response\": response}\n",
    "#\n",
    "#     elif (res.classification == \"specify\") and (res.device == \"none\"):\n",
    "#         print(\"ðŸš« Device is unclear.\")\n",
    "#         update = None\n",
    "#     else:\n",
    "#         raise ValueError(f\"Invalid classification: {res.classification}\")\n",
    "#\n",
    "#     goto = END\n",
    "#     return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3972d75f80bf31c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:24.221292Z",
     "start_time": "2025-04-01T08:17:24.218378Z"
    }
   },
   "source": [
    "# med_agent = StateGraph(State).add_node(medical_device_retriever_router)\n",
    "# med_agent = med_agent.add_edge(START, \"medical_device_retriever_router\")\n",
    "# med_agent = med_agent.compile()\n",
    "#\n",
    "# display(Image(med_agent.get_graph(xray=True).draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "ab18fac97c43b774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:24.606836Z",
     "start_time": "2025-04-01T08:17:24.604662Z"
    }
   },
   "source": [
    "# dummy_res = med_agent.invoke(\n",
    "#     {\"question\": \"Mizuho: how do I troubleshoot Dampened Waveform?\", \"k\": 5}\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "4bd271ac2889e214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T08:17:25.025985Z",
     "start_time": "2025-04-01T08:17:25.023948Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3758ed0511be2cac",
   "metadata": {},
   "source": "## Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d89d728aec9329e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:41:16.445438Z",
     "start_time": "2025-03-26T20:41:14.772152Z"
    }
   },
   "outputs": [],
   "source": [
    "from trulens.core import TruSession, Feedback\n",
    "from trulens.providers.langchain import Langchain\n",
    "from trulens.apps.langchain import TruChain\n",
    "from trulens.dashboard.run import run_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a6c2514c70ec66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:41:16.607201Z",
     "start_time": "2025-03-26T20:41:16.472423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd9e08c6ee6528b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:46:51.863103Z",
     "start_time": "2025-03-26T20:46:51.747841Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_llm = init_chat_model(model=TOGETHER_META_LLAMA_70B_FREE, temperature=0.0)\n",
    "provider = Langchain(chain=eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70b84b54c9a6fd15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:46:52.473534Z",
     "start_time": "2025-03-26T20:46:52.396179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input context will be set to __record__.app.first.steps__.context.first.invoke.rets[:].page_content .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.first.steps__.context.first.invoke.rets[:].page_content.collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "context = TruChain.select_context(rag_chain)\n",
    "\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance, name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    "    # .aggregate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77ac850f854604c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:46:54.578962Z",
     "start_time": "2025-03-26T20:46:53.063204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'langchain_core.runnables.base.RunnableParallel'> for base <class 'langchain_core.runnables.base.RunnableParallel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableParallel'> for base <class 'langchain_core.runnables.base.RunnableSerializable[-Input, dict[str, Any]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableParallel'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableParallel'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting _get_relevant_documents\n",
      "\tinstrumenting get_relevant_documents\n",
      "\tinstrumenting aget_relevant_documents\n",
      "\tinstrumenting _aget_relevant_documents\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.retrievers.BaseRetriever'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting _get_relevant_documents\n",
      "\tinstrumenting get_relevant_documents\n",
      "\tinstrumenting aget_relevant_documents\n",
      "\tinstrumenting _aget_relevant_documents\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.runnables.base.RunnableSerializable[str, list[Document]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.runnables.passthrough.RunnablePassthrough'> for base <class 'langchain_core.runnables.passthrough.RunnablePassthrough'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.passthrough.RunnablePassthrough'> for base <class 'langchain_core.runnables.base.RunnableSerializable[~Other, ~Other]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.passthrough.RunnablePassthrough'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.passthrough.RunnablePassthrough'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.string.StrOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.transform.BaseTransformOutputParser[str]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.transform.BaseTransformOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.base.BaseOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.runnables.base.RunnableSerializable[Union[BaseMessage, str], ~T]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.runnables.base.RunnableSequence'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.chat.BaseChatPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.string.StringPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat._StringImageMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat.BaseMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_together.chat_models.ChatTogether'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_openai.chat_models.base.BaseChatOpenAI'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.language_models.chat_models.BaseChatModel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.language_models.base.BaseLanguageModel[BaseMessage]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.language_models.base.BaseLanguageModel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], ~LanguageModelOutputVar]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_together.chat_models.ChatTogether'> for base <class 'langchain_core.load.serializable.Serializable'>\n"
     ]
    }
   ],
   "source": [
    "recorder_kwargs = {\n",
    "    \"app_name\": \"RAG App\",\n",
    "    \"app_version\": \"0.1.0\",\n",
    "    \"feedbacks\": [f_answer_relevance, f_context_relevance, f_groundedness],\n",
    "}\n",
    "\n",
    "tru_recorder = TruChain(rag_chain, **recorder_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46261300ac2fc620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:47:07.314400Z",
     "start_time": "2025-03-26T20:46:55.346500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:10<00:32, 10.92s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'id': 'nnbtVPr-4Yz4kd-9269804ea97a3bcb', 'error': {'message': 'You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)', 'type': 'model_rate_limit', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtime\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m sleep\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m tqdm(ground_truth_actual_data[-\u001B[32m4\u001B[39m:]):\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtru_recorder\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrecording\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrag_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m# sleep(30)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/app.py:1085\u001B[39m, in \u001B[36mApp.__exit__\u001B[39m\u001B[34m(self, exc_type, exc_value, exc_tb)\u001B[39m\n\u001B[32m   1082\u001B[39m \u001B[38;5;28mself\u001B[39m.recording_contexts.reset(ctx.token)\n\u001B[32m   1084\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m exc_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1085\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value\n\u001B[32m   1087\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m tqdm(ground_truth_actual_data[-\u001B[32m4\u001B[39m:]):\n\u001B[32m      4\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m tru_recorder \u001B[38;5;28;01mas\u001B[39;00m recording:\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m         \u001B[43mrag_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m# sleep(30)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:959\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    956\u001B[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001B[32m    957\u001B[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001B[32m--> \u001B[39m\u001B[32m959\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrewrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrets\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:943\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001B[39m\u001B[34m(rets)\u001B[39m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m python_utils.WRAP_LAZY:\n\u001B[32m    942\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m ctx \u001B[38;5;129;01min\u001B[39;00m contexts:\n\u001B[32m--> \u001B[39m\u001B[32m943\u001B[39m         rets = \u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_lazy_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m            \u001B[49m\u001B[43mwrap\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m            \u001B[49m\u001B[43mon_done\u001B[49m\u001B[43m=\u001B[49m\u001B[43mupdate_call_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcontext_vars\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    950\u001B[39m     update_call_info(rets, final=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:154\u001B[39m, in \u001B[36mWithInstrumentCallbacks.wrap_lazy_values\u001B[39m\u001B[34m(self, rets, wrap, on_done, context_vars)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m python_utils.is_lazy(rets):\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m rets\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mon_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrets\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:887\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001B[39m\u001B[34m(rets, final)\u001B[39m\n\u001B[32m    881\u001B[39m         cost = tally()  \u001B[38;5;66;03m# get updated cost\u001B[39;00m\n\u001B[32m    883\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(stack) == \u001B[32m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m existing_record \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    884\u001B[39m         \u001B[38;5;66;03m# If this is a root call, notify app to add the completed record\u001B[39;00m\n\u001B[32m    885\u001B[39m         \u001B[38;5;66;03m# into its containers:\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m887\u001B[39m         records[ctx] = \u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mon_add_record\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    888\u001B[39m \u001B[43m            \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    889\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    890\u001B[39m \u001B[43m            \u001B[49m\u001B[43msig\u001B[49m\u001B[43m=\u001B[49m\u001B[43msig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    891\u001B[39m \u001B[43m            \u001B[49m\u001B[43mbindings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbindings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m            \u001B[49m\u001B[43mret\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m            \u001B[49m\u001B[43merror\u001B[49m\u001B[43m=\u001B[49m\u001B[43merror\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m            \u001B[49m\u001B[43mperf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_schema\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPerf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_time\u001B[49m\u001B[43m=\u001B[49m\u001B[43mend_time\u001B[49m\n\u001B[32m    896\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    897\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcost\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcost\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexisting_record\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexisting_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    902\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    903\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/app.py:1233\u001B[39m, in \u001B[36mApp.on_add_record\u001B[39m\u001B[34m(self, ctx, func, sig, bindings, ret, error, perf, cost, existing_record, final)\u001B[39m\n\u001B[32m   1230\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1231\u001B[39m     \u001B[38;5;66;03m# May block on DB.\u001B[39;00m\n\u001B[32m   1232\u001B[39m     \u001B[38;5;28mself\u001B[39m._handle_error(record=record, error=error)\n\u001B[32m-> \u001B[39m\u001B[32m1233\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[32m   1235\u001B[39m \u001B[38;5;66;03m# Only continue with the feedback steps if the record is final.\u001B[39;00m\n\u001B[32m   1236\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m final:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:801\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    797\u001B[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001B[32m    799\u001B[39m     logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m801\u001B[39m     rets, tally = \u001B[43mcore_endpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrack_all_costs_tally\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    803\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    805\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    806\u001B[39m     error = e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:570\u001B[39m, in \u001B[36mEndpoint.track_all_costs_tally\u001B[39m\u001B[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    548\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrack_all_costs_tally\u001B[39m(\n\u001B[32m    549\u001B[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001B[32m   (...)\u001B[39m\u001B[32m    557\u001B[39m     **kwargs,\n\u001B[32m    558\u001B[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001B[32m    559\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001B[39;00m\n\u001B[32m    560\u001B[39m \u001B[33;03m    execution of thunk.\u001B[39;00m\n\u001B[32m    561\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    567\u001B[39m \u001B[33;03m            change after this method returns in case of Awaitable results.\u001B[39;00m\n\u001B[32m    568\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m570\u001B[39m     result, cbs = \u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrack_all_costs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m        \u001B[49m\u001B[43m__func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_openai\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_openai\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_hugs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_hugs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    575\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_litellm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_litellm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_bedrock\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_bedrock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_cortex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_cortex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_dummy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_dummy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    580\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    582\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cbs) == \u001B[32m0\u001B[39m:\n\u001B[32m    583\u001B[39m         \u001B[38;5;66;03m# Otherwise sum returns \"0\" below.\u001B[39;00m\n\u001B[32m    584\u001B[39m         tally = \u001B[38;5;28;01mlambda\u001B[39;00m: base_schema.Cost()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:543\u001B[39m, in \u001B[36mEndpoint.track_all_costs\u001B[39m\u001B[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001B[39m\n\u001B[32m    534\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    535\u001B[39m             logger.debug(\n\u001B[32m    536\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mCould not initialize endpoint \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    537\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mPossibly missing key(s). \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    540\u001B[39m                 e,\n\u001B[32m    541\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_track_costs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[43m__func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_endpoints\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:647\u001B[39m, in \u001B[36mEndpoint._track_costs\u001B[39m\u001B[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001B[39m\n\u001B[32m    642\u001B[39m context_vars = {\n\u001B[32m    643\u001B[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001B[32m    644\u001B[39m }\n\u001B[32m    646\u001B[39m \u001B[38;5;66;03m# Call the function.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m647\u001B[39m result: T = \u001B[43m__func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrewrap\u001B[39m(result):\n\u001B[32m    650\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m python_utils.is_lazy(result):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3025\u001B[39m, in \u001B[36mRunnableSequence.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3023\u001B[39m                 \u001B[38;5;28minput\u001B[39m = context.run(step.invoke, \u001B[38;5;28minput\u001B[39m, config, **kwargs)\n\u001B[32m   3024\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3025\u001B[39m                 \u001B[38;5;28minput\u001B[39m = context.run(step.invoke, \u001B[38;5;28minput\u001B[39m, config)\n\u001B[32m   3026\u001B[39m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[32m   3027\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:959\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    956\u001B[39m WithInstrumentCallbacks._stack_contexts.reset(stacks_token)\n\u001B[32m    957\u001B[39m WithInstrumentCallbacks._context_contexts.reset(context_token)\n\u001B[32m--> \u001B[39m\u001B[32m959\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrewrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrets\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:943\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.rewrap\u001B[39m\u001B[34m(rets)\u001B[39m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m python_utils.WRAP_LAZY:\n\u001B[32m    942\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m ctx \u001B[38;5;129;01min\u001B[39;00m contexts:\n\u001B[32m--> \u001B[39m\u001B[32m943\u001B[39m         rets = \u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_lazy_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m            \u001B[49m\u001B[43mwrap\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m            \u001B[49m\u001B[43mon_done\u001B[49m\u001B[43m=\u001B[49m\u001B[43mupdate_call_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcontext_vars\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    950\u001B[39m     update_call_info(rets, final=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:154\u001B[39m, in \u001B[36mWithInstrumentCallbacks.wrap_lazy_values\u001B[39m\u001B[34m(self, rets, wrap, on_done, context_vars)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m python_utils.is_lazy(rets):\n\u001B[32m    152\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m rets\n\u001B[32m--> \u001B[39m\u001B[32m154\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mon_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrets\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:903\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper.<locals>.update_call_info\u001B[39m\u001B[34m(rets, final)\u001B[39m\n\u001B[32m    887\u001B[39m         records[ctx] = ctx.app.on_add_record(\n\u001B[32m    888\u001B[39m             ctx=ctx,\n\u001B[32m    889\u001B[39m             func=func,\n\u001B[32m   (...)\u001B[39m\u001B[32m    899\u001B[39m             final=final,\n\u001B[32m    900\u001B[39m         )\n\u001B[32m    902\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m rets\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/instruments.py:801\u001B[39m, in \u001B[36mInstrument.tracked_method_wrapper.<locals>.tru_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    797\u001B[39m     bindings: BoundArguments = sig.bind(*args, **kwargs)\n\u001B[32m    799\u001B[39m     logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcalling \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m801\u001B[39m     rets, tally = \u001B[43mcore_endpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrack_all_costs_tally\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    802\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    803\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    805\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    806\u001B[39m     error = e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:570\u001B[39m, in \u001B[36mEndpoint.track_all_costs_tally\u001B[39m\u001B[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001B[39m\n\u001B[32m    547\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m    548\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrack_all_costs_tally\u001B[39m(\n\u001B[32m    549\u001B[39m     __func: asynchro_utils.CallableMaybeAwaitable[A, T],\n\u001B[32m   (...)\u001B[39m\u001B[32m    557\u001B[39m     **kwargs,\n\u001B[32m    558\u001B[39m ) -> Tuple[T, python_utils.Thunk[base_schema.Cost]]:\n\u001B[32m    559\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Track costs of all of the apis we can currently track, over the\u001B[39;00m\n\u001B[32m    560\u001B[39m \u001B[33;03m    execution of thunk.\u001B[39;00m\n\u001B[32m    561\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    567\u001B[39m \u001B[33;03m            change after this method returns in case of Awaitable results.\u001B[39;00m\n\u001B[32m    568\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m570\u001B[39m     result, cbs = \u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrack_all_costs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m        \u001B[49m\u001B[43m__func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_openai\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_openai\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_hugs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_hugs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    575\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_litellm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_litellm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_bedrock\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_bedrock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_cortex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_cortex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m        \u001B[49m\u001B[43mwith_dummy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_dummy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    580\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    582\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cbs) == \u001B[32m0\u001B[39m:\n\u001B[32m    583\u001B[39m         \u001B[38;5;66;03m# Otherwise sum returns \"0\" below.\u001B[39;00m\n\u001B[32m    584\u001B[39m         tally = \u001B[38;5;28;01mlambda\u001B[39;00m: base_schema.Cost()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:543\u001B[39m, in \u001B[36mEndpoint.track_all_costs\u001B[39m\u001B[34m(_Endpoint__func, with_openai, with_hugs, with_litellm, with_bedrock, with_cortex, with_dummy, *args, **kwargs)\u001B[39m\n\u001B[32m    534\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    535\u001B[39m             logger.debug(\n\u001B[32m    536\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mCould not initialize endpoint \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    537\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mPossibly missing key(s). \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    540\u001B[39m                 e,\n\u001B[32m    541\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mEndpoint\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_track_costs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m    \u001B[49m\u001B[43m__func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_endpoints\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/trulens/core/feedback/endpoint.py:647\u001B[39m, in \u001B[36mEndpoint._track_costs\u001B[39m\u001B[34m(_Endpoint__func, with_endpoints, *args, **kwargs)\u001B[39m\n\u001B[32m    642\u001B[39m context_vars = {\n\u001B[32m    643\u001B[39m     Endpoint._context_endpoints: Endpoint._context_endpoints.get()\n\u001B[32m    644\u001B[39m }\n\u001B[32m    646\u001B[39m \u001B[38;5;66;03m# Call the function.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m647\u001B[39m result: T = \u001B[43m__func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrewrap\u001B[39m(result):\n\u001B[32m    650\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m python_utils.is_lazy(result):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:307\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    296\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    297\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    298\u001B[39m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[32m   (...)\u001B[39m\u001B[32m    302\u001B[39m     **kwargs: Any,\n\u001B[32m    303\u001B[39m ) -> BaseMessage:\n\u001B[32m    304\u001B[39m     config = ensure_config(config)\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    306\u001B[39m         ChatGeneration,\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    317\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:843\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    835\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    836\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    837\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[32m   (...)\u001B[39m\u001B[32m    840\u001B[39m     **kwargs: Any,\n\u001B[32m    841\u001B[39m ) -> LLMResult:\n\u001B[32m    842\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m843\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:683\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    680\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[32m    681\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    682\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m683\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    684\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    687\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    688\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    689\u001B[39m         )\n\u001B[32m    690\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    691\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:908\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    906\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    907\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m908\u001B[39m         result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    909\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    910\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    911\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    912\u001B[39m         result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:823\u001B[39m, in \u001B[36mBaseChatOpenAI._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     generation_info = {\u001B[33m\"\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response.headers)}\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_chat_result(response, generation_info)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    277\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    278\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:863\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    821\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    823\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    860\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = NOT_GIVEN,\n\u001B[32m    861\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m    862\u001B[39m     validate_response_format(response_format)\n\u001B[32m--> \u001B[39m\u001B[32m863\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    864\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    865\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    866\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    869\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    871\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    872\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    873\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    882\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    883\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    885\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    886\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    887\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    888\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    889\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    890\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    891\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    896\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    897\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    902\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    903\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    904\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    906\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1283\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1269\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1270\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1271\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1278\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1279\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1280\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1281\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1282\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:960\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[39m\n\u001B[32m    957\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    958\u001B[39m     retries_taken = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m960\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1049\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1047\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_retry(err.response):\n\u001B[32m   1048\u001B[39m     err.response.close()\n\u001B[32m-> \u001B[39m\u001B[32m1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[32m   1059\u001B[39m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err.response.is_closed:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1098\u001B[39m, in \u001B[36mSyncAPIClient._retry_request\u001B[39m\u001B[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[39m\n\u001B[32m   1094\u001B[39m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[32m   1095\u001B[39m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[32m   1096\u001B[39m time.sleep(timeout)\n\u001B[32m-> \u001B[39m\u001B[32m1098\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1099\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1100\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1104\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1049\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1047\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._should_retry(err.response):\n\u001B[32m   1048\u001B[39m     err.response.close()\n\u001B[32m-> \u001B[39m\u001B[32m1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[32m   1059\u001B[39m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err.response.is_closed:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1098\u001B[39m, in \u001B[36mSyncAPIClient._retry_request\u001B[39m\u001B[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[39m\n\u001B[32m   1094\u001B[39m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[32m   1095\u001B[39m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[32m   1096\u001B[39m time.sleep(timeout)\n\u001B[32m-> \u001B[39m\u001B[32m1098\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1099\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1100\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1104\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/RAGapp/.venv/lib/python3.11/site-packages/openai/_base_client.py:1064\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1061\u001B[39m         err.response.read()\n\u001B[32m   1063\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_response(\n\u001B[32m   1067\u001B[39m     cast_to=cast_to,\n\u001B[32m   1068\u001B[39m     options=options,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1072\u001B[39m     retries_taken=retries_taken,\n\u001B[32m   1073\u001B[39m )\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'id': 'nnbtVPr-4Yz4kd-9269804ea97a3bcb', 'error': {'message': 'You have reached the rate limit specific to this model meta-llama/Llama-3.3-70B-Instruct-Turbo-Free. The maximum rate limit for this model is 6.0 queries per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)', 'type': 'model_rate_limit', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "for sample in tqdm(ground_truth_actual_data[-4:]):\n",
    "    with tru_recorder as recording:\n",
    "        rag_chain.invoke(sample[\"question\"])\n",
    "    # sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b49f9325d3fcb44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T20:42:13.632974Z",
     "start_time": "2025-03-26T20:42:12.950564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3575f8a0943e41c98c563eb830c866da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:53645 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bd04e6952c608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
