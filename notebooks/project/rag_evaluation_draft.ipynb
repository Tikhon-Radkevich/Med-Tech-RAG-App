{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a048d46b763342db",
   "metadata": {},
   "source": "### Setup"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04e023ceb94c65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:45.254929Z",
     "start_time": "2025-04-20T09:14:45.249040Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"trulens\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:47.120249Z",
     "start_time": "2025-04-20T09:14:45.396675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Optional, Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from trulens.core import TruSession, Select, Feedback, Provider\n",
    "from trulens.core.instruments import instrument\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.dashboard.run import run_dashboard\n",
    "\n",
    "from ragapp.rag.med_agent_graph import (\n",
    "    MedTechAgent,\n",
    "    DeviceEnum,\n",
    "    AgentQueryState,\n",
    "    RelevantDocumentSet,\n",
    ")\n",
    "from constants import (\n",
    "    GROQ_MISTRAL_24B,\n",
    "    TOGETHER_META_LLAMA_70B_FREE,\n",
    "    GROQ_LLAMA_SCOUT_17B,\n",
    "    GROQ_DEEPSEEK,\n",
    "    GROQ_QWEN_32B,\n",
    "    GROQ_LLAMA_70B,\n",
    "    GROUND_TRUTH_ACTUAL_JSON,\n",
    "    EMBEDDINGS_MODEL_NAME,\n",
    "    LP_PLUMBER_CACHE_DIR,\n",
    "    LP_DOCLING_CACHE_DIR,\n",
    "    LP_PLUMBER_COLLECTION_NAME,\n",
    "    LP_DOCLING_COLLECTION_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608b42abc74be5c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:47.133870Z",
     "start_time": "2025-04-20T09:14:47.126866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbaf25ae2986ab",
   "metadata": {},
   "source": "### Load Data && Storages && Agents"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bce1ac62db369f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:47.179308Z",
     "start_time": "2025-04-20T09:14:47.175797Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(GROUND_TRUTH_ACTUAL_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    ground_truth_actual_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8177da582e3f74ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:54.461165Z",
     "start_time": "2025-04-20T09:14:47.225454Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n",
    "\n",
    "docling_storage = Chroma(\n",
    "    collection_name=LP_DOCLING_COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=LP_DOCLING_CACHE_DIR,\n",
    ")\n",
    "plumber_storage = Chroma(\n",
    "    collection_name=LP_PLUMBER_COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=LP_PLUMBER_CACHE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21453951164b6dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.716390Z",
     "start_time": "2025-04-20T09:14:54.469179Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = dict(\n",
    "    query_check_model=GROQ_QWEN_32B,\n",
    "    device_classifier_model=GROQ_QWEN_32B,\n",
    "    paraphraser_model=GROQ_LLAMA_SCOUT_17B,\n",
    "    relevance_selector_model=TOGETHER_META_LLAMA_70B_FREE,\n",
    "    answer_generator_model=GROQ_LLAMA_70B,\n",
    "    error_handler_model=GROQ_LLAMA_70B,\n",
    ")\n",
    "k_value = 5\n",
    "\n",
    "plumber_med_agent = MedTechAgent(\n",
    "    vector_storage=plumber_storage, k=k_value, **model_names\n",
    ")\n",
    "docling_med_agent = MedTechAgent(\n",
    "    vector_storage=docling_storage, k=k_value, **model_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e6a9a0c902522",
   "metadata": {},
   "source": "### Specify Eval Samples"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83039a04f710fb42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.732538Z",
     "start_time": "2025-04-20T09:14:55.726787Z"
    }
   },
   "outputs": [],
   "source": [
    "def explore_sample(storage, sample, device=None, search_pattern: str = None):\n",
    "    print(\"Question:\", sample[\"question\"], \"\\n\\n\")\n",
    "    print(\"Ground Truth:\\n\", sample[\"ground_truth\"], \"\\n\\n\\n\")\n",
    "\n",
    "    get_res = storage.get(\n",
    "        where_document={\"$contains\": search_pattern}, where={\"device\": device}\n",
    "    )\n",
    "\n",
    "    n_docs = len(get_res[\"ids\"])\n",
    "    print(\"#\" * 20, f\"DOCUMENTS FOUND ({n_docs})\", \"#\" * 20)\n",
    "    print(f\"\\t search pattern: '{search_pattern}'\")\n",
    "    print(\"\\t device filter:\", device)\n",
    "    for i, doc in enumerate(get_res[\"documents\"], start=1):\n",
    "        print(\"#\" * 20, f\"Doc {i}\", \"#\" * 20)\n",
    "        print(doc)\n",
    "\n",
    "    print(\"#\" * 20, \"END DOCUMENTS:\", \"#\" * 20)\n",
    "\n",
    "    start_point = sample[\"ground_truth\"].rfind(search_pattern)\n",
    "\n",
    "    print(\"\\n\\n\\nSEARCH PATTERN EDGES IN GROUND TRUTH:\")\n",
    "    print(f\"\\t Pattern edges: {start_point}:{start_point + len(search_pattern)}\")\n",
    "    print(f\"\\t Pattern: '{search_pattern}'\")\n",
    "    print(\"\\nDOCS: [ pdf title | page ]\")\n",
    "    for i, doc_metadata in enumerate(get_res[\"metadatas\"], start=1):\n",
    "        print(\"\\t\", doc_metadata[\"pdf_title\"], \"|\", doc_metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abdf2bf5656bda7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.779577Z",
     "start_time": "2025-04-20T09:14:55.775756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in speaker grill holes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_actual_data[10][\"ground_truth\"][89:111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b4d96178f1a1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.827456Z",
     "start_time": "2025-04-20T09:14:55.824551Z"
    }
   },
   "outputs": [],
   "source": [
    "## Docling missed one item from the table:\n",
    "\n",
    "# doc = docling_storage.get(\n",
    "#     where={\n",
    "#         \"$and\": [\n",
    "#             {\"device\": DeviceEnum.lifepak_20},\n",
    "#             {\"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\"},\n",
    "#             {\"page\": \"123\"}\n",
    "#         ]\n",
    "#     }\n",
    "# )\n",
    "#\n",
    "# print(doc[\"documents\"][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39a2d3f979658d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.898527Z",
     "start_time": "2025-04-20T09:14:55.873610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I troubleshoot low volume on Lifepak 20? \n",
      "\n",
      "\n",
      "Ground Truth:\n",
      " Possible Cause and Corrective Action \n",
      "A Possible cause of low speaker volume is moisture in speaker grill holes. To resolve, wipe moisture from speaker grill and allow device to dry. \n",
      "\n",
      "\n",
      "\n",
      "#################### DOCUMENTS FOUND (1) ####################\n",
      "\t search pattern: 'in speaker grill holes'\n",
      "\t device filter: DeviceEnum.lifepak_20\n",
      "#################### Doc 1 ####################\n",
      "GENERAL TROUBLESHOOTING TIPS\n",
      "Table 7-2 General Troubleshooting Tips\n",
      "GLYPH<129> Refer to Section 4, page 4-20.. 9 Problems with pacing., Possible Cause = . 9 Problems with pacing., Corrective Action = GLYPH<129> Refer to Section 4, page 4-22.. 10 Displayed time is incorrect., Possible Cause = Time is incorrectly set.. 10 Displayed time is incorrect., Corrective Action = GLYPH<129> Change the time setting. Refer to Section 2, page 2-7.. 11 Date printed on report is incorrect., Possible Cause = Date is incorrectly set.. 11 Date printed on report is incorrect., Corrective Action = GLYPH<129> Change the date setting. Refer to Section 2, page 2-7.. 12 Displayed messages are faint or flicker., Possible Cause = Low battery power. Out of temperature range.. 12 Displayed messages are faint or flicker., Corrective Action = GLYPH<129> Connect to AC power immediately.. 13 Low speaker volume., Possible Cause = Moisture in speaker grill holes.. 13 Low speaker volume., Corrective Action = GLYPH<129> Wipe moisture from speaker grill and allow device to dry.\n",
      "#################### END DOCUMENTS: ####################\n",
      "\n",
      "\n",
      "\n",
      "SEARCH PATTERN EDGES IN GROUND TRUTH:\n",
      "\t Pattern edges: 89:111\n",
      "\t Pattern: 'in speaker grill holes'\n",
      "\n",
      "DOCS: [ pdf title | page ]\n",
      "\t Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf | 110\n"
     ]
    }
   ],
   "source": [
    "# eval_ids = [0, 1, 10, 11, 8]\n",
    "\n",
    "## sample 10\n",
    "explore_sample(\n",
    "    storage=docling_storage,\n",
    "    sample=ground_truth_actual_data[10],\n",
    "    device=DeviceEnum.lifepak_20,\n",
    "    search_pattern=ground_truth_actual_data[10][\"ground_truth\"][89:111],\n",
    ")\n",
    "\n",
    "## sample 0\n",
    "# explore_sample(\n",
    "#     storage=docling_storage,\n",
    "#     sample=ground_truth_actual_data[0],\n",
    "#     device=DeviceEnum.lifepak_15,\n",
    "#     search_pattern=ground_truth_actual_data[0][\"ground_truth\"][26:118],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4caeefb2ed256c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:55.931596Z",
     "start_time": "2025-04-20T09:14:55.924409Z"
    }
   },
   "outputs": [],
   "source": [
    "test = [\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[0][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[0][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[0][\"ground_truth\"][26:118],\n",
    "        \"device\": DeviceEnum.lifepak_15,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_LIFEPAK_15_Monitor_Defibrillator_3314911_036_202105.pdf\",\n",
    "                \"pages\": [37],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[1][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[1][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[1][\"ground_truth\"][34:57],\n",
    "        \"device\": DeviceEnum.lifepak_15,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_LIFEPAK_15_Monitor_Defibrillator_3314911_036_202105.pdf\",\n",
    "                \"pages\": [90],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[10][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[10][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[10][\"ground_truth\"][89:111],\n",
    "        \"device\": DeviceEnum.lifepak_20,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf\",\n",
    "                \"pages\": [110],\n",
    "            },\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\",\n",
    "                \"pages\": [123],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[11][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[11][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[11][\"ground_truth\"][239:267],\n",
    "        \"device\": DeviceEnum.lifepak_20,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf\",\n",
    "                \"pages\": [65],\n",
    "            },\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\",\n",
    "                \"pages\": [74],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72e51a65ab3c23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:14:56.088022Z",
     "start_time": "2025-04-20T09:14:55.978966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert (1/1)\n",
      "assert (1/1)\n",
      "assert (1/1)\n",
      "assert (1/1)\n",
      "assert (1/2)\n",
      "assert (1/2)\n",
      "\n",
      "AssertionError (2/2):\n",
      "Storage name: docling\n",
      "pdf_title: Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\n",
      "device: LIFEPAK 20\n",
      "page: 123\n",
      "content: in speaker grill holes \n",
      "\n",
      "assert (2/2)\n",
      "assert (1/2)\n",
      "assert (1/2)\n",
      "assert (2/2)\n",
      "assert (2/2)\n"
     ]
    }
   ],
   "source": [
    "## Asserts that the theoretical document metadata matches the metadata retrieved from vector storage.\n",
    "storages = {\n",
    "    \"docling\": docling_storage,\n",
    "    \"plumber\": plumber_storage,\n",
    "}\n",
    "\n",
    "for t in test:\n",
    "    test_docs = t[\"docs\"]\n",
    "    for i_test, d in enumerate(test_docs, start=1):\n",
    "        for page in d[\"pages\"]:\n",
    "            where_filter = {\n",
    "                \"$and\": [\n",
    "                    {\"device\": t[\"device\"].value},\n",
    "                    {\"pdf_title\": d[\"pdf_title\"]},\n",
    "                    {\"page\": page},\n",
    "                ]\n",
    "            }\n",
    "            for storage_name, storage in storages.items():\n",
    "                res = storage.get(\n",
    "                    where_document={\"$contains\": t[\"search_pattern\"]},\n",
    "                    where=where_filter,\n",
    "                )\n",
    "                try:\n",
    "                    assert len(res[\"ids\"]) == 1\n",
    "                    print(f\"assert ({i_test}/{len(test_docs)})\")\n",
    "                except AssertionError:\n",
    "                    print(f\"\\nAssertionError ({i_test}/{len(test_docs)}):\")\n",
    "                    print(\"Storage name:\", storage_name)\n",
    "                    print(\"pdf_title:\", d[\"pdf_title\"])\n",
    "                    print(\"device:\", t[\"device\"].value)\n",
    "                    print(\"page:\", page)\n",
    "                    print(\"content:\", t[\"search_pattern\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97777b7dfa667bf9",
   "metadata": {},
   "source": "### Run Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0e044b1f1417ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:05.353847Z",
     "start_time": "2025-04-20T09:37:05.258272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ae624bcb054688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:15:02.192621Z",
     "start_time": "2025-04-20T09:15:02.187348Z"
    }
   },
   "outputs": [],
   "source": [
    "instrument.method(MedTechAgent, \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b227be44946311d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:07.238832Z",
     "start_time": "2025-04-20T09:37:07.229231Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_answer_relevance = \"\"\"\n",
    "You are an expert evaluator for medical device technical documentation. Your task is to assess the accuracy and completeness of an AI-generated response compared to the provided ground truth answer. You will assign a score from 0 to 3 based on the given criteria.\n",
    "\n",
    "### **Input Variables:**\n",
    "- **Question**: The user's query asking about a medical device's technical details or instructions.\n",
    "- **Ground Truth Answer**: The expected correct response, usually a structured set of instructions with bullet points or numbered steps.\n",
    "- **Answer**: The AI-generated response that needs to be evaluated.\n",
    "\n",
    "### **Scoring Criteria:**\n",
    "- **0 (Incorrect Answer)**: The **Answer** is mostly or entirely incorrect. It may be unrelated to the **Question**, reference the wrong instructions, or address a different process.\n",
    "- **1 (Partially Correct)**: The **Answer** contains some correct points but is incomplete or includes incorrect elements. It may also be **completely unstructured**, making it difficult to follow.\n",
    "- **2 (Mostly Correct but Unstructured)**: The **Answer** is factually correct and includes all required points from the **Ground Truth Answer**, but it is not formatted properly (e.g., lacks bullet points, numbering, or section headers).\n",
    "- **3 (Fully Correct and Well-Structured)**: The **Answer** is entirely correct, contains all required points, and follows the same structured format as the **Ground Truth Answer** (e.g., numbered steps, bullet points, section headers).\n",
    "\n",
    "### **Evaluation Steps:**\n",
    "1. Compare the **content accuracy** of the **Answer** to the **Ground Truth Answer**.\n",
    "2. Check whether all key points from the **Ground Truth Answer** are included in the **Answer**.\n",
    "3. Assess the **structure and formatting** (e.g., bullet points, numbered lists, sections).\n",
    "4. Assign a score from 0 to 3 based on the above criteria.\n",
    "\n",
    "### **Output Format:**\n",
    "Provide your evaluation as follows:\n",
    "- **Score:** [0-3]\n",
    "\"\"\"\n",
    "# todo: add support for returning reasoning\n",
    "# - **Reasoning:** Explain why you assigned this score, highlighting missing, incorrect, or unstructured elements.\n",
    "\n",
    "system_prompt_groundedness = \"\"\"\n",
    "You are an expert evaluator of technical documentation for medical devices. Your task is to assess whether an AI-generated response groundedness information—that is, whether it introduces content not supported by the provided context. Assign a score from 0 to 2 based on the criteria below.\n",
    "\n",
    "### Input Variables:\n",
    "- **Relevant Context**: Retrieved content from the documentation database that the AI had access to when generating its response.\n",
    "- **Answer**: The AI-generated response to be evaluated for hallucination.\n",
    "\n",
    "### Scoring Criteria:\n",
    "- **0 — Hallucinated Answer**: The **Answer** is not based on the **Relevant Context** and includes fabricated or unsupported content.\n",
    "- **1 — Partially Grounded**: The **Answer** is partially based on the **Relevant Context**, but includes additional details, reworded instructions, or differs in structure.\n",
    "- **2 — Fully Grounded**: The **Answer** is entirely consistent with the **Relevant Context**, both in content and structure.\n",
    "\n",
    "### Special Cases:\n",
    "- If no **Relevant Context** is provided and the AI clearly states that it cannot answer due to a lack of information, score **2**.\n",
    "- If no **Relevant Context** is provided and the AI still attempts to answer, score **0**.\n",
    "\n",
    "### **Output Format:**\n",
    "Provide your evaluation as follows:\n",
    "- **Score:** [0-2]\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_relevant_context_selection = \"\"\"\n",
    "You are an expert evaluator of technical documentation for medical devices. Your task is to assess whether an AI system selected the correct relevant context from a set of retrieved documentation chunks to answer a given question.\n",
    "You must assign a score from 0 to 2 based on the criteria below.\n",
    "\n",
    "### Input Variables:\n",
    "- **Retrieved Context Chunks**: All chunks retrieved from the documentation database for the current question.\n",
    "- **Correct Context Chunks**: The chunks known to contain the information required to correctly answer the question.\n",
    "- **Relevant Context Selected**: The subset of **Retrieved Context Chunks** that the AI selected as relevant for answering the question.\n",
    "\n",
    "### Scoring Criteria:\n",
    "- **Score 0**:\n",
    "  - The **Retrieved Context Chunks** include the **Correct Context Chunks**, but the AI selected **only irrelevant chunks**.\n",
    "  - Or: The **Correct Context Chunks** are **not** in the retrieved set, but the AI still selected unrelated chunks as relevant.\n",
    "\n",
    "- **Score 1**:\n",
    "  - The AI selected a mix of correct and incorrect context chunks. That is, **some** of the **Correct Context Chunks** were selected, but not all.\n",
    "\n",
    "- **Score 2**:\n",
    "  - The AI selected **all** and **only** the **Correct Context Chunks** from the retrieved set.\n",
    "  - Or: The **Correct Context Chunks** were **not** present in the **Retrieved Context Chunks**, and the AI correctly selected **nothing** as relevant.\n",
    "\n",
    "### **Output Format:**\n",
    "Provide your evaluation as follows:\n",
    "- **Score:** [0-2]\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_relevant_context_selection = \"\"\"\n",
    "**Retrieved Context Chunks**:\n",
    "{retrieved_context}\n",
    "\n",
    "\n",
    "**Correct Context Chunks**:\n",
    "{correct_context}\n",
    "\n",
    "\n",
    "**Relevant Context Selected**:\n",
    "{relevant_context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_groundedness = \"\"\"\n",
    "Relevant Context: {context}\n",
    "\n",
    "Answer: {answer}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_answer_relevance = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth Answer: {ground_truth_answer}\n",
    "\n",
    "\n",
    "Answer: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a4ec750aecdd11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:08.096075Z",
     "start_time": "2025-04-20T09:37:08.069547Z"
    }
   },
   "outputs": [],
   "source": [
    "class StandAlone(Provider):\n",
    "    class DocMeta(BaseModel):\n",
    "        \"\"\"Doc metadata used for matching AI responses with test docs.\"\"\"\n",
    "\n",
    "        pdf_title: str = Field(description=\"PDF title.\")\n",
    "        page: int = Field(description=\"Page number.\")\n",
    "        device: DeviceEnum = Field(description=\"Device type.\")\n",
    "        content: Optional[str] = Field(default=None)\n",
    "\n",
    "        def str_preview(self, index: int) -> str:\n",
    "            return f\"### Doc ({index}) ### \\npdf title: {self.pdf_title} \\npage: {self.page}\"\n",
    "\n",
    "        def __hash__(self):\n",
    "            return hash((self.pdf_title, self.page, self.device))\n",
    "\n",
    "        def __eq__(self, other):\n",
    "            return (\n",
    "                self.pdf_title == other.pdf_title\n",
    "                and self.page == other.page\n",
    "                and self.device == other.device\n",
    "            )\n",
    "\n",
    "    class EvaluatorLLMSchema(BaseModel):\n",
    "        score: float = Field(default=None, description=\"The metric score of answer.\")\n",
    "        # reasoning: str = Field(description=\"The reasoning of score.\")  # todo: add support for returning reasoning\n",
    "\n",
    "    answer_relevance_llm: Any\n",
    "    retrieve_func: Any\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        answer_relevance_llm: Runnable,\n",
    "        retrieve_func: Callable[[str, int, str], list[Document]],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.answer_relevance_llm = answer_relevance_llm\n",
    "        self.retrieve_func = retrieve_func\n",
    "\n",
    "    @staticmethod\n",
    "    def device_classification(response: dict) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        actual_device = state.state_kwargs[\"device\"]\n",
    "        device = state.device_classification.device\n",
    "        return actual_device == device\n",
    "\n",
    "    def relevant_context_selection(self, response: dict) -> float:\n",
    "        \"\"\"Process retrieved, selected and actual docs\"\"\"\n",
    "        state = AgentQueryState(**response)\n",
    "        actual_doc_meta = self._get_actual_doc_meta(state.state_kwargs)\n",
    "        response_doc_meta = self._get_docs_meta(state)\n",
    "        relevant_selected = state.filtered_relevant_documents.relevant_sources\n",
    "\n",
    "        actual_doc_str = \"\\n\\n\".join(\n",
    "            [doc.str_preview(i) for i, doc in enumerate(actual_doc_meta, start=1)]\n",
    "        )\n",
    "        response_doc_str = \"\\n\\n\".join(\n",
    "            [doc.str_preview(i) for i, doc in enumerate(response_doc_meta, start=1)]\n",
    "        )\n",
    "\n",
    "        relevant_selected_list = []\n",
    "        for relevant_selected_doc in relevant_selected:\n",
    "            title = relevant_selected_doc.title\n",
    "            pages = \", \".join(map(str, relevant_selected_doc.pages))\n",
    "            relevant_selected_list.append(f\"pdf title: {title} \\npages: {pages}\")\n",
    "\n",
    "        relevant_selected_str = \"\\n\\n\".join(relevant_selected_list)\n",
    "\n",
    "        user_prompt_formatted = user_prompt_relevant_context_selection.format(\n",
    "            retrieved_context=response_doc_str,\n",
    "            correct_context=actual_doc_str,\n",
    "            relevant_context=relevant_selected_str,\n",
    "        )\n",
    "        llm_inputs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_relevant_context_selection},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "        ]\n",
    "        score = self.answer_relevance_llm.invoke(llm_inputs).score\n",
    "        return self._normalize_score(score, 2)\n",
    "\n",
    "    def context_recall(self, response: dict) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        response_doc_meta = self._get_docs_meta(state.retrieved_documents)\n",
    "        relevant_doc_meta = self._get_relevant_docs_meta(state)\n",
    "        actual_doc_meta = self._get_actual_doc_meta(state.state_kwargs)\n",
    "\n",
    "        positives = response_doc_meta & actual_doc_meta\n",
    "        if not positives:\n",
    "            return 1\n",
    "\n",
    "        true_positives = len(positives & relevant_doc_meta)\n",
    "        return true_positives / len(positives)\n",
    "\n",
    "    def context_precision(self, response: dict) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        response_doc_meta = self._get_docs_meta(state.retrieved_documents)\n",
    "        relevant_doc_meta = self._get_relevant_docs_meta(state)\n",
    "        actual_doc_meta = self._get_actual_doc_meta(state.state_kwargs)\n",
    "\n",
    "        positives = response_doc_meta & actual_doc_meta\n",
    "        true_positives = len(positives & relevant_doc_meta) or 1\n",
    "\n",
    "        negatives = response_doc_meta - actual_doc_meta\n",
    "        false_positives = len(negatives & relevant_doc_meta)\n",
    "\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "    def groundedness(self, response: dict) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        docs_metadata = self._get_docs_meta(state.retrieved_documents)\n",
    "        actual_docs_metadata = self._get_actual_doc_meta(state.state_kwargs)\n",
    "\n",
    "        relevant_content = \"\\n\".join(\n",
    "            [\n",
    "                doc_meta.content\n",
    "                for doc_meta in docs_metadata\n",
    "                if doc_meta in actual_docs_metadata\n",
    "            ]\n",
    "        )\n",
    "        relevant_content = relevant_content or \"None\"\n",
    "\n",
    "        user_prompt_formatted = user_prompt_groundedness.format(\n",
    "            context=relevant_content, answer=state.final_response\n",
    "        )\n",
    "\n",
    "        llm_inputs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_groundedness},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "        ]\n",
    "        score = self.answer_relevance_llm.invoke(llm_inputs).score\n",
    "        return self._normalize_score(score, 2)\n",
    "\n",
    "    def answer_relevance(self, response: dict) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        user_prompt_formatted = user_prompt_answer_relevance.format(\n",
    "            question=state.paraphrased_question,\n",
    "            ground_truth_answer=state.state_kwargs[\"ground_truth\"],\n",
    "            answer=state.final_response,\n",
    "        )\n",
    "\n",
    "        llm_inputs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_answer_relevance},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "        ]\n",
    "\n",
    "        score = self.answer_relevance_llm.invoke(llm_inputs).score\n",
    "        return self._normalize_score(score, 3)\n",
    "\n",
    "    def vector_store_recall(self, response, k) -> float:\n",
    "        state = AgentQueryState(**response)\n",
    "        actual_doc_meta = self._get_actual_doc_meta(state.state_kwargs)\n",
    "        k = max(k, len(actual_doc_meta))\n",
    "        query = state.paraphrased_question\n",
    "        device = state.device_classification.device.value\n",
    "        docs = self.retrieve_func(query, k, device)\n",
    "        docs_meta = self._get_docs_meta(docs)\n",
    "        true_positives = docs_meta & actual_doc_meta\n",
    "        return len(true_positives) / len(actual_doc_meta)\n",
    "\n",
    "    def _get_relevant_docs_meta(self, state: AgentQueryState) -> set[DocMeta]:\n",
    "        device = state.device_classification.device\n",
    "        docs_metadata = set()\n",
    "        for doc in state.filtered_relevant_documents.relevant_sources:\n",
    "            for doc_page in doc.pages:\n",
    "                doc_meta = self.DocMeta(\n",
    "                    pdf_title=doc.title,\n",
    "                    page=int(doc_page),\n",
    "                    device=device,\n",
    "                )\n",
    "                docs_metadata.add(doc_meta)\n",
    "        return docs_metadata\n",
    "\n",
    "    def _get_docs_meta(self, context_docs: list[Document]) -> set[DocMeta]:\n",
    "        # context_docs = [doc for doc in state.retrieved_documents]\n",
    "        docs_metadata = set()\n",
    "        for doc in context_docs:\n",
    "            dm = doc.metadata\n",
    "            doc_meta = self.DocMeta(\n",
    "                pdf_title=dm[\"pdf_title\"],\n",
    "                page=int(dm[\"page\"]),\n",
    "                device=dm[\"device\"],\n",
    "                content=doc.page_content,\n",
    "            )\n",
    "            docs_metadata.add(doc_meta)\n",
    "        return docs_metadata\n",
    "\n",
    "    def _get_actual_doc_meta(self, state_kwargs: dict) -> set[DocMeta]:\n",
    "        actual_docs_metadata = set()\n",
    "        for doc in state_kwargs[\"docs\"]:\n",
    "            for doc_page in doc[\"pages\"]:\n",
    "                device_title_page = self.DocMeta(\n",
    "                    pdf_title=doc[\"pdf_title\"],\n",
    "                    page=int(doc_page),\n",
    "                    device=state_kwargs[\"device\"],\n",
    "                )\n",
    "                actual_docs_metadata.add(device_title_page)\n",
    "        return actual_docs_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_score(score, max_score):\n",
    "        return min(1, max(0, score / max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5977501f49e1f97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:09.354510Z",
     "start_time": "2025-04-20T09:37:09.240613Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_llm = init_chat_model(model=GROQ_LLAMA_SCOUT_17B, temperature=0.0)\n",
    "eval_llm_structured = eval_llm.with_structured_output(StandAlone.EvaluatorLLMSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6753a2fa8e79cec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:09.936137Z",
     "start_time": "2025-04-20T09:37:09.932194Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Test Answer Relevance Metric ###\n",
    "#\n",
    "# user_prompt_formatted = user_prompt_answer_relevance.format(\n",
    "#     question=test[0][\"question\"],\n",
    "#     ground_truth_answer=test[0][\"ground_truth\"],\n",
    "#     answer=\"To replace the battery on the Lifepak 15, first confirm that the new battery is fully charged. Then, inspect the battery pins and contacts in the battery wells for signs of damage. Next, align the new battery so the battery clip is over the pins in the battery well, insert the end of the battery opposite the clip into the well, and press the clip end into the well until it clicks into place. It is recommended to replace batteries approximately every two years or when they show signs of damage or reduced capacity.\"\n",
    "# )\n",
    "#\n",
    "#\n",
    "# llm_inputs = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt_answer_relevance},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "# ]\n",
    "#\n",
    "# res = eval_llm_structured.invoke(llm_inputs)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7f28bbfff897b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:43.251994Z",
     "start_time": "2025-04-20T09:37:43.241462Z"
    }
   },
   "outputs": [],
   "source": [
    "def form_feedbacks(provider: StandAlone):\n",
    "    feedbacks = [\n",
    "        Feedback(provider.device_classification, name=\"Device Classification\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.context_recall, name=\"Context Recall\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.context_precision, name=\"Context Precision\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.answer_relevance, name=\"Answer Relevance\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.groundedness, name=\"Groundedness\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.relevant_context_selection, name=\"Context Selected\").on(\n",
    "            Select.RecordCalls.run.rets\n",
    "        ),\n",
    "        Feedback(provider.vector_store_recall, name=\"VStorage Recall k=2\").on(\n",
    "            response=Select.RecordCalls.run.rets,\n",
    "            k=Select.RecordCalls.run.args.kwargs.vector_storage_k_test.k2,\n",
    "        ),\n",
    "        Feedback(provider.vector_store_recall, name=\"VStorage Recall k=10\").on(\n",
    "            response=Select.RecordCalls.run.rets,\n",
    "            k=Select.RecordCalls.run.args.kwargs.vector_storage_k_test.k10,\n",
    "        ),\n",
    "        Feedback(provider.vector_store_recall, name=\"VStorage Recall k=25\").on(\n",
    "            response=Select.RecordCalls.run.rets,\n",
    "            k=Select.RecordCalls.run.args.kwargs.vector_storage_k_test.k25,\n",
    "        ),\n",
    "    ]\n",
    "    return feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6399ba68d09a0ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:43.650513Z",
     "start_time": "2025-04-20T09:37:43.646864Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(agent: MedTechAgent, eval_recorder_kwargs: dict):\n",
    "    tru_recorder = TruApp(agent, **eval_recorder_kwargs)\n",
    "\n",
    "    for sample in tqdm(test):\n",
    "        inputs = {\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"device\": sample[\"device\"],\n",
    "            \"docs\": sample[\"docs\"],\n",
    "            \"vector_storage_k_test\": {\n",
    "                \"k2\": 2,\n",
    "                \"k10\": 10,\n",
    "                \"k25\": 25,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        ## Respect API rate limits\n",
    "        # start = time.time()\n",
    "\n",
    "        with tru_recorder as _:\n",
    "            _ = agent.run(**inputs)\n",
    "\n",
    "        # time_sleep = 30 - (time.time() - start)\n",
    "        # if time_sleep > 0:\n",
    "        #     time.sleep(time_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4bd471ef8e1c4167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:37:45.562660Z",
     "start_time": "2025-04-20T09:37:45.368729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Device Classification, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Recall, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Precision, input response will be set to __record__.app.run.rets .\n",
      "✅ In Answer Relevance, input response will be set to __record__.app.run.rets .\n",
      "✅ In Groundedness, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Selected, input response will be set to __record__.app.run.rets .\n",
      "✅ In Device Classification, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Recall, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Precision, input response will be set to __record__.app.run.rets .\n",
      "✅ In Answer Relevance, input response will be set to __record__.app.run.rets .\n",
      "✅ In Groundedness, input response will be set to __record__.app.run.rets .\n",
      "✅ In Context Selected, input response will be set to __record__.app.run.rets .\n"
     ]
    }
   ],
   "source": [
    "plumber_feedbacks_provider = StandAlone(eval_llm_structured, plumber_med_agent.retrieve)\n",
    "docling_feedbacks_provider = StandAlone(eval_llm_structured, docling_med_agent.retrieve)\n",
    "\n",
    "plumber_recorder_kwargs = {\n",
    "    \"app_name\": \"RAG App Plumber\",\n",
    "    \"feedbacks\": form_feedbacks(plumber_feedbacks_provider),\n",
    "}\n",
    "docling_recorder_kwargs = {\n",
    "    \"app_name\": \"RAG App Docling\",\n",
    "    \"feedbacks\": form_feedbacks(docling_feedbacks_provider),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6c8c1c04fd327ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:39:29.910412Z",
     "start_time": "2025-04-20T09:37:47.457229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'ragapp.rag.med_agent_graph.MedTechAgent'> for base <class 'ragapp.rag.med_agent_graph.MedTechAgent'>\n",
      "\tinstrumenting run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:42<00:00, 25.57s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent(plumber_med_agent, plumber_recorder_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39aa1cf89751ed1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:43:43.020091Z",
     "start_time": "2025-04-20T09:42:09.849673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'ragapp.rag.med_agent_graph.MedTechAgent'> for base <class 'ragapp.rag.med_agent_graph.MedTechAgent'>\n",
      "\tinstrumenting run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:33<00:00, 23.25s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent(docling_med_agent, docling_recorder_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2380e1bcd6d0485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:44:24.046995Z",
     "start_time": "2025-04-20T09:44:23.998459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Device Classification</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>VStorage Recall k=10</th>\n",
       "      <th>VStorage Recall k=2</th>\n",
       "      <th>VStorage Recall k=25</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAG App Docling</th>\n",
       "      <th>base</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.75</td>\n",
       "      <td>22.998292</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAG App Plumber</th>\n",
       "      <th>base</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>25.315782</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Answer Relevance  Context Precision  \\\n",
       "app_name        app_version                                        \n",
       "RAG App Docling base                     0.75           0.354167   \n",
       "RAG App Plumber base                     0.50           0.333333   \n",
       "\n",
       "                             Context Recall  Device Classification  \\\n",
       "app_name        app_version                                          \n",
       "RAG App Docling base                    1.0                    1.0   \n",
       "RAG App Plumber base                    1.0                    1.0   \n",
       "\n",
       "                             Groundedness  VStorage Recall k=10  \\\n",
       "app_name        app_version                                       \n",
       "RAG App Docling base                 0.75                 0.625   \n",
       "RAG App Plumber base                 0.75                 0.500   \n",
       "\n",
       "                             VStorage Recall k=2  VStorage Recall k=25  \\\n",
       "app_name        app_version                                              \n",
       "RAG App Docling base                       0.625                  0.75   \n",
       "RAG App Plumber base                       0.500                  0.50   \n",
       "\n",
       "                               latency  total_cost  \n",
       "app_name        app_version                         \n",
       "RAG App Docling base         22.998292         0.0  \n",
       "RAG App Plumber base         25.315782         0.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75c2f7a41e43fd67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:42:01.983491Z",
     "start_time": "2025-04-14T07:42:01.246625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf6d0b97e7a4026ba45a505208aeb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://localhost:56629 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f93068274be27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315a17104e9c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
