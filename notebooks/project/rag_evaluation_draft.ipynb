{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a048d46b763342db",
   "metadata": {},
   "source": "### Setup"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:33.566840Z",
     "start_time": "2025-04-09T09:20:31.795738Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from trulens.core import TruSession, Select, Feedback, Provider\n",
    "from trulens.core.instruments import instrument\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.dashboard.run import run_dashboard\n",
    "\n",
    "from ragapp.rag.med_agent_graph import MedTechAgent, DeviceEnum\n",
    "from constants import (\n",
    "    TOGETHER_META_LLAMA_70B_FREE,\n",
    "    GROQ_LLAMA_90B,\n",
    "    GROQ_GEMMA_9B,\n",
    "    GROUND_TRUTH_ACTUAL_JSON,\n",
    "    EMBEDDINGS_MODEL_NAME,\n",
    "    LP_PLUMBER_CACHE_DIR,\n",
    "    LP_DOCLING_CACHE_DIR,\n",
    "    LP_PLUMBER_COLLECTION_NAME,\n",
    "    LP_DOCLING_COLLECTION_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608b42abc74be5c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:36.820774Z",
     "start_time": "2025-04-09T09:20:36.813035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbaf25ae2986ab",
   "metadata": {},
   "source": "### Load Data && Storages && Agents"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bce1ac62db369f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:37.882422Z",
     "start_time": "2025-04-09T09:20:37.874174Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(GROUND_TRUTH_ACTUAL_JSON, \"r\", encoding=\"utf-8\") as file:\n",
    "    ground_truth_actual_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8177da582e3f74ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:46.950243Z",
     "start_time": "2025-04-09T09:20:40.009104Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)\n",
    "\n",
    "docling_storage = Chroma(\n",
    "    collection_name=LP_DOCLING_COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=LP_DOCLING_CACHE_DIR,\n",
    ")\n",
    "plumber_storage = Chroma(\n",
    "    collection_name=LP_PLUMBER_COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=LP_PLUMBER_CACHE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21453951164b6dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:48.315236Z",
     "start_time": "2025-04-09T09:20:47.537500Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_mode_name = TOGETHER_META_LLAMA_70B_FREE\n",
    "device_model_name = GROQ_GEMMA_9B\n",
    "\n",
    "plumber_med_agent = MedTechAgent(\n",
    "    vector_storage=plumber_storage,\n",
    "    rag_model_name=chat_mode_name,\n",
    "    device_model_name=device_model_name,\n",
    ")\n",
    "\n",
    "docling_med_agent = MedTechAgent(\n",
    "    vector_storage=docling_storage,\n",
    "    rag_model_name=chat_mode_name,\n",
    "    device_model_name=device_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e6a9a0c902522",
   "metadata": {},
   "source": "### Specify Eval Samples"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83039a04f710fb42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:48.331511Z",
     "start_time": "2025-04-09T09:20:48.325878Z"
    }
   },
   "outputs": [],
   "source": [
    "def explore_sample(storage, sample, device=None, search_pattern: str = None):\n",
    "    print(\"Question:\", sample[\"question\"], \"\\n\\n\")\n",
    "    print(\"Ground Truth:\\n\", sample[\"ground_truth\"], \"\\n\\n\\n\")\n",
    "\n",
    "    get_res = storage.get(\n",
    "        where_document={\"$contains\": search_pattern}, where={\"device\": device}\n",
    "    )\n",
    "\n",
    "    n_docs = len(get_res[\"ids\"])\n",
    "    print(\"#\" * 20, f\"DOCUMENTS FOUND ({n_docs})\", \"#\" * 20)\n",
    "    print(f\"\\t search pattern: '{search_pattern}'\")\n",
    "    print(\"\\t device filter:\", device)\n",
    "    for i, doc in enumerate(get_res[\"documents\"], start=1):\n",
    "        print(\"#\" * 20, f\"Doc {i}\", \"#\" * 20)\n",
    "        print(doc)\n",
    "\n",
    "    print(\"#\" * 20, \"END DOCUMENTS:\", \"#\" * 20)\n",
    "\n",
    "    start_point = sample[\"ground_truth\"].rfind(search_pattern)\n",
    "\n",
    "    print(\"\\n\\n\\nSEARCH PATTERN EDGES IN GROUND TRUTH:\")\n",
    "    print(f\"\\t Pattern edges: {start_point}:{start_point + len(search_pattern)}\")\n",
    "    print(f\"\\t Pattern: '{search_pattern}'\")\n",
    "    print(\"\\nDOCS: [ pdf title | page ]\")\n",
    "    for i, doc_metadata in enumerate(get_res[\"metadatas\"], start=1):\n",
    "        print(\"\\t\", doc_metadata[\"pdf_title\"], \"|\", doc_metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39a2d3f979658d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:50.143998Z",
     "start_time": "2025-04-09T09:20:50.122926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I troubleshoot low volume on Lifepak 20? \n",
      "\n",
      "\n",
      "Ground Truth:\n",
      " Possible Cause and Corrective Action \n",
      "A Possible cause of low speaker volume is moisture in speaker grill holes. To resolve, wipe moisture from speaker grill and allow device to dry. \n",
      "\n",
      "\n",
      "\n",
      "#################### DOCUMENTS FOUND (1) ####################\n",
      "\t search pattern: 'in speaker grill holes'\n",
      "\t device filter: DeviceEnum.lifepak_20\n",
      "#################### Doc 1 ####################\n",
      "GENERAL TROUBLESHOOTING TIPS\n",
      "Table 7-2 General Troubleshooting Tips\n",
      "GLYPH<129> Refer to Section 4, page 4-20.. 9 Problems with pacing., Possible Cause = . 9 Problems with pacing., Corrective Action = GLYPH<129> Refer to Section 4, page 4-22.. 10 Displayed time is incorrect., Possible Cause = Time is incorrectly set.. 10 Displayed time is incorrect., Corrective Action = GLYPH<129> Change the time setting. Refer to Section 2, page 2-7.. 11 Date printed on report is incorrect., Possible Cause = Date is incorrectly set.. 11 Date printed on report is incorrect., Corrective Action = GLYPH<129> Change the date setting. Refer to Section 2, page 2-7.. 12 Displayed messages are faint or flicker., Possible Cause = Low battery power. Out of temperature range.. 12 Displayed messages are faint or flicker., Corrective Action = GLYPH<129> Connect to AC power immediately.. 13 Low speaker volume., Possible Cause = Moisture in speaker grill holes.. 13 Low speaker volume., Corrective Action = GLYPH<129> Wipe moisture from speaker grill and allow device to dry.\n",
      "#################### END DOCUMENTS: ####################\n",
      "\n",
      "\n",
      "\n",
      "SEARCH PATTERN EDGES IN GROUND TRUTH:\n",
      "\t Pattern edges: 89:111\n",
      "\t Pattern: 'in speaker grill holes'\n",
      "\n",
      "DOCS: [ pdf title | page ]\n",
      "\t Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf | 111\n"
     ]
    }
   ],
   "source": [
    "# eval_ids = [0, 1, 10, 11, 8]\n",
    "\n",
    "## sample 10\n",
    "explore_sample(\n",
    "    storage=docling_storage,\n",
    "    sample=ground_truth_actual_data[10],\n",
    "    device=DeviceEnum.lifepak_20,\n",
    "    search_pattern=ground_truth_actual_data[10][\"ground_truth\"][89:111],\n",
    ")\n",
    "\n",
    "## sample 0\n",
    "# explore_sample(\n",
    "#     storage=docling_storage,\n",
    "#     sample=ground_truth_actual_data[0],\n",
    "#     device=DeviceEnum.lifepak_15,\n",
    "#     search_pattern=ground_truth_actual_data[0][\"ground_truth\"][26:118],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4caeefb2ed256c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:50.927323Z",
     "start_time": "2025-04-09T09:20:50.914816Z"
    }
   },
   "outputs": [],
   "source": [
    "test = [\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[0][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[0][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[0][\"ground_truth\"][26:118],\n",
    "        \"device\": DeviceEnum.lifepak_15,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_LIFEPAK_15_Monitor_Defibrillator_3314911_036_202105.pdf\",\n",
    "                \"pages\": [37],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[1][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[1][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[1][\"ground_truth\"][34:57],\n",
    "        \"device\": DeviceEnum.lifepak_15,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_LIFEPAK_15_Monitor_Defibrillator_3314911_036_202105.pdf\",\n",
    "                \"pages\": [90],\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[10][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[10][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[10][\"ground_truth\"][89:111],\n",
    "        \"device\": DeviceEnum.lifepak_20,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf\",\n",
    "                \"pages\": [110],\n",
    "            },\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\",\n",
    "                \"pages\": [123],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": ground_truth_actual_data[11][\"question\"],\n",
    "        \"ground_truth\": ground_truth_actual_data[11][\"ground_truth\"],\n",
    "        \"search_pattern\": ground_truth_actual_data[11][\"ground_truth\"][239:267],\n",
    "        \"device\": DeviceEnum.lifepak_20,\n",
    "        \"docs\": [\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3200750_039_201404_eq00.pdf\",\n",
    "                \"pages\": [65],\n",
    "            },\n",
    "            {\n",
    "                \"pdf_title\": \"Stryker_Physio_Control_Lifepak_20_Defibrillator_Monitor_3313180_008_201508.pdf\",\n",
    "                \"pages\": [74],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72e51a65ab3c23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:51.915227Z",
     "start_time": "2025-04-09T09:20:51.852404Z"
    }
   },
   "outputs": [],
   "source": [
    "## Asserts that the theoretical document metadata matches the metadata retrieved from vector storage.\n",
    "\n",
    "for t in test:\n",
    "    for d in t[\"docs\"]:\n",
    "        for page in d[\"pages\"]:\n",
    "            where_filter = {\n",
    "                \"$and\": [\n",
    "                    {\"device\": t[\"device\"].value},\n",
    "                    {\"pdf_title\": d[\"pdf_title\"]},\n",
    "                    {\"page\": page},\n",
    "                ]\n",
    "            }\n",
    "            # todo: check also `docling_storage`\n",
    "            res = plumber_storage.get(\n",
    "                where_document={\"$contains\": t[\"search_pattern\"]},\n",
    "                where=where_filter,\n",
    "            )\n",
    "            assert len(res[\"ids\"]) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97777b7dfa667bf9",
   "metadata": {},
   "source": "### Run Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e044b1f1417ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:54.729641Z",
     "start_time": "2025-04-09T09:20:54.592952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ae624bcb054688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:55.580695Z",
     "start_time": "2025-04-09T09:20:55.577653Z"
    }
   },
   "outputs": [],
   "source": [
    "instrument.method(MedTechAgent, \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b227be44946311d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:56.170577Z",
     "start_time": "2025-04-09T09:20:56.164248Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_answer_relevance = \"\"\"\n",
    "You are an expert evaluator for medical device technical documentation. Your task is to assess the accuracy and completeness of an AI-generated response compared to the provided ground truth answer. You will assign a score from 0 to 3 based on the given criteria.\n",
    "\n",
    "### **Input Variables:**\n",
    "- **Question**: The user's query asking about a medical device's technical details or instructions.\n",
    "- **Ground Truth Answer**: The expected correct response, usually a structured set of instructions with bullet points or numbered steps.\n",
    "- **Answer**: The AI-generated response that needs to be evaluated.\n",
    "\n",
    "### **Scoring Criteria:**\n",
    "- **0 (Incorrect Answer)**: The **Answer** is mostly or entirely incorrect. It may be unrelated to the **Question**, reference the wrong instructions, or address a different process.\n",
    "- **1 (Partially Correct)**: The **Answer** contains some correct points but is incomplete or includes incorrect elements. It may also be **completely unstructured**, making it difficult to follow.\n",
    "- **2 (Mostly Correct but Unstructured)**: The **Answer** is factually correct and includes all required points from the **Ground Truth Answer**, but it is not formatted properly (e.g., lacks bullet points, numbering, or section headers).\n",
    "- **3 (Fully Correct and Well-Structured)**: The **Answer** is entirely correct, contains all required points, and follows the same structured format as the **Ground Truth Answer** (e.g., numbered steps, bullet points, section headers).\n",
    "\n",
    "### **Evaluation Steps:**\n",
    "1. Compare the **content accuracy** of the **Answer** to the **Ground Truth Answer**.\n",
    "2. Check whether all key points from the **Ground Truth Answer** are included in the **Answer**.\n",
    "3. Assess the **structure and formatting** (e.g., bullet points, numbered lists, sections).\n",
    "4. Assign a score from 0 to 3 based on the above criteria.\n",
    "\n",
    "### **Output Format:**\n",
    "Provide your evaluation as follows:\n",
    "- **Score:** [0-3]\n",
    "\"\"\"\n",
    "# todo: add support for returning reasoning\n",
    "# - **Reasoning:** Explain why you assigned this score, highlighting missing, incorrect, or unstructured elements.\n",
    "\n",
    "system_prompt_hallucination = \"\"\"\n",
    "You are an expert evaluator of technical documentation for medical devices. Your task is to assess whether an AI-generated response hallucinates informationâ€”that is, whether it introduces content not supported by the provided context. Assign a score from 0 to 2 based on the criteria below.\n",
    "\n",
    "### Input Variables:\n",
    "- **Relevant Context**: Retrieved content from the documentation database that the AI had access to when generating its response.\n",
    "- **Answer**: The AI-generated response to be evaluated for hallucination.\n",
    "\n",
    "### Scoring Criteria:\n",
    "- **0 â€” Hallucinated Answer**: The **Answer** is not based on the **Relevant Context** and includes fabricated or unsupported content.\n",
    "- **1 â€” Partially Grounded**: The **Answer** is partially based on the **Relevant Context**, but includes additional details, reworded instructions, or differs in structure.\n",
    "- **2 â€” Fully Grounded**: The **Answer** is entirely consistent with the **Relevant Context**, both in content and structure.\n",
    "\n",
    "### Special Cases:\n",
    "- If no **Relevant Context** is provided and the AI clearly states that it cannot answer due to a lack of information, score **2**.\n",
    "- If no **Relevant Context** is provided and the AI still attempts to answer, score **0**.\n",
    "\n",
    "### **Output Format:**\n",
    "Provide your evaluation as follows:\n",
    "- **Score:** [0-2]\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_hallucination = \"\"\"\n",
    "Relevant Context: {context}\n",
    "\n",
    "Answer: {answer}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_answer_relevance = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth Answer: {ground_truth_answer}\n",
    "\n",
    "\n",
    "Answer: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a4ec750aecdd11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:57.105977Z",
     "start_time": "2025-04-09T09:20:57.089886Z"
    }
   },
   "outputs": [],
   "source": [
    "class StandAlone(Provider):\n",
    "    class DocMeta(BaseModel):\n",
    "        \"\"\"Doc metadata used for matching AI responses with test docs.\"\"\"\n",
    "\n",
    "        pdf_title: str = Field(description=\"PDF title.\")\n",
    "        page: int = Field(description=\"Page number.\")\n",
    "        device: DeviceEnum = Field(description=\"Device type.\")\n",
    "        content: Optional[str] = Field(default=None)\n",
    "\n",
    "        def __hash__(self):\n",
    "            return hash((self.pdf_title, self.page, self.device))\n",
    "\n",
    "        def __eq__(self, other):\n",
    "            return (\n",
    "                self.pdf_title == other.pdf_title\n",
    "                and self.page == other.page\n",
    "                and self.device == other.device\n",
    "            )\n",
    "\n",
    "    class EvaluatorLLMSchema(BaseModel):\n",
    "        score: float = Field(default=None, description=\"The metric score of answer.\")\n",
    "        # reasoning: str = Field(description=\"The reasoning of score.\")  # todo: add support for returning reasoning\n",
    "\n",
    "    answer_relevance_llm: Any = Field(default=None)\n",
    "    is_docling_storage: bool = Field(default=False)\n",
    "\n",
    "    def __init__(self, answer_relevance_llm, is_docling_storage=False):\n",
    "        super().__init__()\n",
    "        self.answer_relevance_llm = answer_relevance_llm\n",
    "        self.is_docling_storage = is_docling_storage\n",
    "\n",
    "    def context_provided(self, response: dict) -> float:\n",
    "        response_doc_meta = self._get_docs_meta(response)\n",
    "        actual_doc_meta = self._get_actual_doc_meta(response[\"state_kwargs\"])\n",
    "\n",
    "        doc_contains = any([data in actual_doc_meta for data in response_doc_meta])\n",
    "        return doc_contains\n",
    "\n",
    "    def hallucination(self, response: dict) -> float:\n",
    "        docs_metadata = self._get_docs_meta(response)\n",
    "        actual_docs_metadata = self._get_actual_doc_meta(response[\"state_kwargs\"])\n",
    "\n",
    "        relevant_content = \"\\n\".join(\n",
    "            [\n",
    "                doc_meta.content\n",
    "                for doc_meta in docs_metadata\n",
    "                if doc_meta in actual_docs_metadata\n",
    "            ]\n",
    "        )\n",
    "        relevant_content = relevant_content or \"None\"\n",
    "\n",
    "        user_prompt_formatted = user_prompt_hallucination.format(\n",
    "            context=relevant_content,\n",
    "            answer=response[\"retrieval_result\"][\"answer\"],\n",
    "        )\n",
    "\n",
    "        llm_inputs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_hallucination},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "        ]\n",
    "        score = self.answer_relevance_llm.invoke(llm_inputs).score\n",
    "        return score / 2\n",
    "\n",
    "    def answer_relevance(self, response: dict) -> float:\n",
    "        user_prompt_formatted = user_prompt_answer_relevance.format(\n",
    "            question=response[\"question\"],\n",
    "            ground_truth_answer=response[\"state_kwargs\"][\"ground_truth\"],\n",
    "            answer=response[\"retrieval_result\"][\"answer\"],\n",
    "        )\n",
    "\n",
    "        llm_inputs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_answer_relevance},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "        ]\n",
    "\n",
    "        score = self.answer_relevance_llm.invoke(llm_inputs).score\n",
    "        return score / 3\n",
    "\n",
    "    def _get_docs_meta(self, response: dict) -> set[DocMeta]:\n",
    "        context_docs = [doc for doc in response[\"context_documents\"]]\n",
    "        docs_metadata = set()\n",
    "        for doc in context_docs:\n",
    "            dm = doc[\"metadata\"]\n",
    "            doc_meta = self.DocMeta(\n",
    "                pdf_title=dm[\"pdf_title\"],\n",
    "                page=int(dm[\"page\"]),\n",
    "                device=dm[\"device\"],\n",
    "                content=doc[\"page_content\"],\n",
    "            )\n",
    "            docs_metadata.add(doc_meta)\n",
    "        return docs_metadata\n",
    "\n",
    "    def _get_actual_doc_meta(self, state_kwargs) -> set[DocMeta]:\n",
    "        actual_docs_metadata = set()\n",
    "        for doc in state_kwargs[\"docs\"]:\n",
    "            for p in doc[\"pages\"]:\n",
    "                doc_page = int(p)\n",
    "                if self.is_docling_storage:\n",
    "                    doc_page += 1  # issue with Docling page offset\n",
    "                device_title_page = self.DocMeta(\n",
    "                    pdf_title=doc[\"pdf_title\"],\n",
    "                    page=doc_page,\n",
    "                    device=state_kwargs[\"device\"],\n",
    "                )\n",
    "                actual_docs_metadata.add(device_title_page)\n",
    "        return actual_docs_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5977501f49e1f97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:58.210584Z",
     "start_time": "2025-04-09T09:20:58.111763Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_llm = init_chat_model(model=GROQ_LLAMA_90B, temperature=0.0)\n",
    "eval_llm_structured = eval_llm.with_structured_output(StandAlone.EvaluatorLLMSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6753a2fa8e79cec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:20:58.869105Z",
     "start_time": "2025-04-09T09:20:58.865380Z"
    }
   },
   "outputs": [],
   "source": [
    "### Test Answer Relevance Metric ###\n",
    "\n",
    "# user_prompt_formatted = user_prompt_answer_relevance.format(\n",
    "#     question=test[0][\"question\"],\n",
    "#     ground_truth_answer=test[0][\"ground_truth\"],\n",
    "#     answer=\"To replace the battery on the Lifepak 15, first confirm that the new battery is fully charged. Then, inspect the battery pins and contacts in the battery wells for signs of damage. Next, align the new battery so the battery clip is over the pins in the battery well, insert the end of the battery opposite the clip into the well, and press the clip end into the well until it clicks into place. It is recommended to replace batteries approximately every two years or when they show signs of damage or reduced capacity.\"\n",
    "# )\n",
    "#\n",
    "#\n",
    "# llm_inputs = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt_answer_relevance},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt_formatted},\n",
    "# ]\n",
    "#\n",
    "# res = eval_llm_structured.invoke(llm_inputs)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f28bbfff897b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:21:02.789092Z",
     "start_time": "2025-04-09T09:21:02.782706Z"
    }
   },
   "outputs": [],
   "source": [
    "def form_feedbacks(provider: StandAlone):\n",
    "    f_context_relevance = Feedback(\n",
    "        provider.context_provided, name=\"Context Relevance\"\n",
    "    ).on(Select.RecordCalls.run.rets)\n",
    "\n",
    "    f_answer_relevance = Feedback(\n",
    "        provider.answer_relevance, name=\"Answer Relevance\"\n",
    "    ).on(Select.RecordCalls.run.rets)\n",
    "\n",
    "    f_hallucination = Feedback(provider.hallucination, name=\"Hallucination\").on(\n",
    "        Select.RecordCalls.run.rets\n",
    "    )\n",
    "    return [f_context_relevance, f_answer_relevance, f_hallucination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6399ba68d09a0ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:21:04.009217Z",
     "start_time": "2025-04-09T09:21:04.001958Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(agent: MedTechAgent, eval_recorder_kwargs: dict):\n",
    "    tru_recorder = TruApp(agent, **eval_recorder_kwargs)\n",
    "\n",
    "    for sample in tqdm(test):\n",
    "        inputs = {\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"k\": 3,\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"device\": sample[\"device\"],\n",
    "            \"docs\": sample[\"docs\"],\n",
    "        }\n",
    "\n",
    "        ## Respect API rate limits\n",
    "        # start = time.time()\n",
    "\n",
    "        with tru_recorder as recording:\n",
    "            _ = agent.run(**inputs)\n",
    "\n",
    "        # time_sleep = 30 - (time.time() - start)\n",
    "        # if time_sleep > 0:\n",
    "        #     time.sleep(time_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bd471ef8e1c4167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:21:04.682117Z",
     "start_time": "2025-04-09T09:21:04.608722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Context Relevance, input response will be set to __record__.app.run.rets .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.app.run.rets .\n",
      "âœ… In Hallucination, input response will be set to __record__.app.run.rets .\n",
      "âœ… In Context Relevance, input response will be set to __record__.app.run.rets .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.app.run.rets .\n",
      "âœ… In Hallucination, input response will be set to __record__.app.run.rets .\n"
     ]
    }
   ],
   "source": [
    "plumber_provider = StandAlone(eval_llm_structured)\n",
    "\n",
    "plumber_recorder_kwargs = {\n",
    "    \"app_name\": \"RAG App Plumber\",\n",
    "    \"app_version\": \"0.1.0\",\n",
    "    \"feedbacks\": form_feedbacks(plumber_provider),\n",
    "}\n",
    "docling_recorder_kwargs = {\n",
    "    \"app_name\": \"RAG App Docling\",\n",
    "    \"app_version\": \"0.1.0\",\n",
    "    \"feedbacks\": form_feedbacks(\n",
    "        StandAlone(eval_llm_structured, is_docling_storage=True)\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6c8c1c04fd327ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:21:44.113959Z",
     "start_time": "2025-04-09T09:21:05.459736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'ragapp.rag.med_agent_graph.MedTechAgent'> for base <class 'ragapp.rag.med_agent_graph.MedTechAgent'>\n",
      "\tinstrumenting run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:38<00:00,  9.64s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent(plumber_med_agent, plumber_recorder_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39aa1cf89751ed1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:22:15.046169Z",
     "start_time": "2025-04-09T09:21:44.152398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'ragapp.rag.med_agent_graph.MedTechAgent'> for base <class 'ragapp.rag.med_agent_graph.MedTechAgent'>\n",
      "\tinstrumenting run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:30<00:00,  7.70s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluate_agent(docling_med_agent, docling_recorder_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2380e1bcd6d0485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:22:15.091835Z",
     "start_time": "2025-04-09T09:22:15.056733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Hallucination</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAG App Docling</th>\n",
       "      <th>0.1.0</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7.604631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAG App Plumber</th>\n",
       "      <th>0.1.0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375</td>\n",
       "      <td>9.535199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Answer Relevance  Context Relevance  \\\n",
       "app_name        app_version                                        \n",
       "RAG App Docling 0.1.0                0.888889               0.75   \n",
       "RAG App Plumber 0.1.0                0.500000               0.50   \n",
       "\n",
       "                             Hallucination   latency  total_cost  \n",
       "app_name        app_version                                       \n",
       "RAG App Docling 0.1.0                1.000  7.604631         0.0  \n",
       "RAG App Plumber 0.1.0                0.375  9.535199         0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2f7a41e43fd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f93068274be27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
